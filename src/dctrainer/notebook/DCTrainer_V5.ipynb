{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCTrainer_V5-alpha.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PkUZho8ERPFR",
        "S9ExsLYHVhCY",
        "7cFeLMGeVwiY",
        "4RXODUFqWd7n",
        "F49jCa4fW_m6",
        "jdadli4NbNNg",
        "YctDhHb2bWcj",
        "zpLFvnIBba_A",
        "ubbUR7U5boZv",
        "WX5FwQujcA-B",
        "QAVdDXrhceYS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('venv': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "interpreter": {
      "hash": "c81fb4fae5f87c8eb51788965fc791026fdabb1e29866ab90609522b08ea234e"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJc2Zr05PezS"
      },
      "source": [
        "Donkey Car Tainer V5\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkUZho8ERPFR"
      },
      "source": [
        "## Avant propos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ExsLYHVhCY"
      },
      "source": [
        "#### Lister les GPU disponibles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pZarMsXVkqF"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtxKQ8FTVuC0"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "print(get_available_gpus())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cFeLMGeVwiY"
      },
      "source": [
        "#### Choisir le GPU de lancement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bg0Vn3cVDKU"
      },
      "source": [
        "- Avec la variable d'environnement `CUDA_VISIBLE_DEVICES`, asigner la valeur :\n",
        "  * `-1` pour faire du calcul sur CPU\n",
        "  * `0` ou `1` ou ... pour faire du calcul respectivement sur GPU 0, 1 ou ...\n",
        "  * `0,1` pour faire du calcul sur les 2 GPU 0 et GPU 1.\n",
        "\n",
        "- Si `CUDA_VISIBLE_DEVICES` ne fonctionne pas,\n",
        "on peut encadrer le fit avec ce code :\n",
        "```\n",
        "with tf.device(\"/gpu:1\"):\n",
        "    model.fit(...)\n",
        "```\n",
        "pour lancer sur le GPU 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-X8eo7CU6u9"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w1yMYAUQsSB"
      },
      "source": [
        "## Importer les librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz27lSPGPMDR"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import inspect\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime\n",
        "from loguru import logger"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dctrainer.utils.utils import build_log_tag\n",
        "from dctrainer.dataset.extractor import ESLRExtractor\n",
        "from dctrainer.dataset.augmentator import DonkeyCarDataAugmentator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RXODUFqWd7n"
      },
      "source": [
        "## Importer la dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F49jCa4fW_m6"
      },
      "source": [
        "### Depuis Colab via Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjtcYCQ0W-Kc"
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c33tTwZ8XaNz"
      },
      "source": [
        "!rm -Rf \"corentin_renault_20000_record_controller\"\n",
        "!cp \"drive/My Drive/ColabStorage/DonkeyCar/Simulator/Dataset/corentin_renault_20000_record_controller.eslr\" \"dataset.eslr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbqbGWrXv28"
      },
      "source": [
        "### Autre source via CURL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAuXDFszXbfz"
      },
      "source": [
        "!curl <LIEN_URL> --output dataset.eslr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O1j1jzTYAHY"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UVlhD7gbEGL"
      },
      "source": [
        "TIME = str(time())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdadli4NbNNg"
      },
      "source": [
        "### Environement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvKreHPDbSj3"
      },
      "source": [
        "### ENVIRONEMENT ###\n",
        "STORAGE_ROOT_DIR = os.path.abspath(\"../../../data\")\n",
        "STORAGE_ROOT_DIR"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/nigiva/git/donkey-car-trainer/data'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YctDhHb2bWcj"
      },
      "source": [
        "### Modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYGz9WCFbYsk"
      },
      "source": [
        "### MODEL ###\n",
        "MODEL_NAME = \"DCDeepModelV5.0-reda-renault-speed_accel_gyro-\" + TIME\n",
        "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
        "os.environ[\"MODEL_NAME_TAR\"] = MODEL_NAME+\".tar.gz\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLFvnIBba_A"
      },
      "source": [
        "### Sauvegarde"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DrIGM1XbmD3"
      },
      "source": [
        "### SAVE PATH ###\n",
        "SAVE_PATH = os.path.join(STORAGE_ROOT_DIR, \"model\", MODEL_NAME)\n",
        "os.environ[\"SAVE_PATH\"] = SAVE_PATH\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Don't remove the last \"s\" in \"checkpoints\",\n",
        "# the file `checkpoint` already exists\n",
        "CHECKPOINT_PATH = os.path.join(SAVE_PATH, \"checkpoints\")\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "CHECKPOINT_FILEPATH = os.path.join(CHECKPOINT_PATH, \"checkpoint-{epoch:02d}.weight\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubbUR7U5boZv"
      },
      "source": [
        "### Log d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjTOmMHBby9-"
      },
      "source": [
        "### LOG ###\n",
        "ROOT_TENSORLOG_PATH = os.path.join(STORAGE_ROOT_DIR, \"log\", MODEL_NAME)\n",
        "os.makedirs(ROOT_TENSORLOG_PATH, exist_ok=True)\n",
        "logger.add(os.path.join(ROOT_TENSORLOG_PATH, \"notebook.log\"), level=\"DEBUG\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i-QICSobzZz"
      },
      "source": [
        "def get_new_tensorlog_path():\n",
        "    special_log_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    special_log_path = os.path.join(ROOT_TENSORLOG_PATH, special_log_name)\n",
        "    os.makedirs(special_log_path)\n",
        "    print(build_log_tag(LOG_PATH=LOG_PATH))\n",
        "    os.environ[\"LOG_PATH\"] = special_log_path\n",
        "    return special_log_path"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX5FwQujcA-B"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyGaEHvPcDLS"
      },
      "source": [
        "### DATASET ###\n",
        "DATASET_NAME = \"corentin_renault_30000_clean_record_controller\"\n",
        "DATASET_FILE_PATH = os.path.join(STORAGE_ROOT_DIR, \"sample\", DATASET_NAME + \".eslr\")\n",
        "\n",
        "IMAGE_PATH = \"images\"\n",
        "DATASET_LABEL_FILENAME = \"label.csv\"\n",
        "\n",
        "DATASET_LABEL_PATH = os.path.join(DATASET_NAME, DATASET_LABEL_FILENAME)\n",
        "DATASET_IMAGE_PATH = os.path.join(DATASET_NAME, IMAGE_PATH)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qc7D5BqcY2h"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "SPLIT_VALIDATION = 0.05\n",
        "SPLIT_TEST = 0.05"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAVdDXrhceYS"
      },
      "source": [
        "### Entrée du réseau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo1Tw6qpcsj4"
      },
      "source": [
        "IMAGE_SHAPE = (120,160, 3)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvbR5ssYc7PU"
      },
      "source": [
        "### Résumé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD9-h62MYGJt"
      },
      "source": [
        "logger.info(build_log_tag(TIME=TIME))\n",
        "logger.info(build_log_tag(STORAGE_ROOT_DIR=STORAGE_ROOT_DIR))\n",
        "logger.info(build_log_tag(MODEL_NAME=MODEL_NAME))\n",
        "logger.info(build_log_tag(SAVE_PATH=SAVE_PATH))\n",
        "logger.info(build_log_tag(CHECKPOINT_PATH=CHECKPOINT_PATH))\n",
        "logger.info(build_log_tag(CHECKPOINT_FILEPATH=CHECKPOINT_FILEPATH))\n",
        "logger.info(build_log_tag(ROOT_TENSORLOG_PATH=ROOT_TENSORLOG_PATH))\n",
        "logger.info(build_log_tag(DATASET_NAME=DATASET_NAME))\n",
        "logger.info(build_log_tag(DATASET_FILE_PATH=DATASET_FILE_PATH))\n",
        "logger.info(build_log_tag(IMAGE_PATH=IMAGE_PATH))\n",
        "logger.info(build_log_tag(DATASET_LABEL_FILENAME=DATASET_LABEL_FILENAME))\n",
        "logger.info(build_log_tag(DATASET_LABEL_PATH=DATASET_LABEL_PATH))\n",
        "logger.info(build_log_tag(DATASET_IMAGE_PATH=DATASET_IMAGE_PATH))\n",
        "logger.info(build_log_tag(BATCH_SIZE=BATCH_SIZE))\n",
        "logger.info(build_log_tag(SPLIT_VALIDATION=SPLIT_VALIDATION))\n",
        "logger.info(build_log_tag(SPLIT_TEST=SPLIT_TEST))\n",
        "logger.info(build_log_tag(IMAGE_SHAPE=IMAGE_SHAPE))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-08-22 20:35:57.425 | INFO     | __main__:<module>:1 - [TIME=\"1629656599.7211332\"]\n",
            "2021-08-22 20:35:57.427 | INFO     | __main__:<module>:2 - [STORAGE_ROOT_DIR=\"/home/nigiva/git/donkey-car-trainer/data\"]\n",
            "2021-08-22 20:35:57.428 | INFO     | __main__:<module>:3 - [MODEL_NAME=\"DCDeepModelV5.0-reda-renault-speed_accel_gyro-1629656599.7211332\"]\n",
            "2021-08-22 20:35:57.429 | INFO     | __main__:<module>:4 - [SAVE_PATH=\"/home/nigiva/git/donkey-car-trainer/data/model/DCDeepModelV5.0-reda-renault-speed_accel_gyro-1629656599.7211332\"]\n",
            "2021-08-22 20:35:57.430 | INFO     | __main__:<module>:5 - [CHECKPOINT_PATH=\"/home/nigiva/git/donkey-car-trainer/data/model/DCDeepModelV5.0-reda-renault-speed_accel_gyro-1629656599.7211332/checkpoints\"]\n",
            "2021-08-22 20:35:57.430 | INFO     | __main__:<module>:6 - [CHECKPOINT_FILEPATH=\"/home/nigiva/git/donkey-car-trainer/data/model/DCDeepModelV5.0-reda-renault-speed_accel_gyro-1629656599.7211332/checkpoints/checkpoint-{epoch:02d}.weight\"]\n",
            "2021-08-22 20:35:57.431 | INFO     | __main__:<module>:7 - [ROOT_TENSORLOG_PATH=\"/home/nigiva/git/donkey-car-trainer/data/log/DCDeepModelV5.0-reda-renault-speed_accel_gyro-1629656599.7211332\"]\n",
            "2021-08-22 20:35:57.432 | INFO     | __main__:<module>:8 - [DATASET_NAME=\"corentin_renault_30000_clean_record_controller\"]\n",
            "2021-08-22 20:35:57.432 | INFO     | __main__:<module>:9 - [DATASET_FILE_PATH=\"/home/nigiva/git/donkey-car-trainer/data/sample/corentin_renault_30000_clean_record_controller.eslr\"]\n",
            "2021-08-22 20:35:57.433 | INFO     | __main__:<module>:10 - [IMAGE_PATH=\"images\"]\n",
            "2021-08-22 20:35:57.435 | INFO     | __main__:<module>:11 - [DATASET_LABEL_FILENAME=\"label.csv\"]\n",
            "2021-08-22 20:35:57.438 | INFO     | __main__:<module>:12 - [DATASET_LABEL_PATH=\"corentin_renault_30000_clean_record_controller/label.csv\"]\n",
            "2021-08-22 20:35:57.439 | INFO     | __main__:<module>:13 - [DATASET_IMAGE_PATH=\"corentin_renault_30000_clean_record_controller/images\"]\n",
            "2021-08-22 20:35:57.440 | INFO     | __main__:<module>:14 - [BATCH_SIZE=\"256\"]\n",
            "2021-08-22 20:35:57.441 | INFO     | __main__:<module>:15 - [SPLIT_VALIDATION=\"0.05\"]\n",
            "2021-08-22 20:35:57.442 | INFO     | __main__:<module>:16 - [SPLIT_TEST=\"0.05\"]\n",
            "2021-08-22 20:35:57.442 | INFO     | __main__:<module>:17 - [IMAGE_SHAPE=\"(120, 160, 3)\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6nfD7xgiAC"
      },
      "source": [
        "## Extraire la dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmE-7IUUgtGE"
      },
      "source": [
        "On convertit chaque ligne du fichier *.eslr envoyées par le serveur en :\n",
        "- une image qui sera stockée dans le dossier `<DATASET_NAME>/<DATASET_IMAGE_PATH>`\n",
        "- une ligne dans le csv label.csv avec toutes les infos (reliées à aux images par leur `path`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKWPJMf7g1Mj"
      },
      "source": [
        "eslr_extractor = ESLRExtractor(DATASET_FILE_PATH)\n",
        "eslr_extractor.extract(label_path = DATASET_LABEL_PATH, images_path = DATASET_IMAGE_PATH)\n",
        "raw_data = eslr_extractor.read_csv(DATASET_LABEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA32IFleg2z1"
      },
      "source": [
        "raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiWJF3Z8g_aR"
      },
      "source": [
        "raw_data.hist(figsize=(20,20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IYJ17buhEFN"
      },
      "source": [
        "## Préparer la dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tya6PW9yhjcn"
      },
      "source": [
        "### Split en 3 jeux : Train, Test et Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLAJtdPJhTTJ"
      },
      "source": [
        "train_and_test_set, validation_set = train_test_split(raw_data,\n",
        "                                             test_size = SPLIT_VALIDATION,\n",
        "                                             shuffle = True)\n",
        "train_set, test_set = train_test_split(train_and_test_set,\n",
        "                                             test_size = SPLIT_TEST,\n",
        "                                             shuffle = True)\n",
        "\n",
        "NBR_ROW_TRAIN_SET = train_set.shape[0]\n",
        "NBR_ROW_TEST_SET = test_set.shape[0]\n",
        "NBR_ROW_VALIDATION_SET = validation_set.shape[0]\n",
        "logger.info(build_log_tag(NBR_ROW_TRAIN_SET=NBR_ROW_TRAIN_SET))\n",
        "logger.info(build_log_tag(NBR_ROW_TEST_SET=NBR_ROW_TEST_SET))\n",
        "logger.info(build_log_tag(NBR_ROW_VALIDATION_SET=NBR_ROW_VALIDATION_SET))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhzCiSAFhtnO"
      },
      "source": [
        "### Traitements avec TensorData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weSexOwciear"
      },
      "source": [
        "#### Donkey Car Tensor Builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLRvD9K5igX1"
      },
      "source": [
        "class DonkeyCarTensorBuilder:\n",
        "  def __init__(self, input_label = {'input':['path']}, output_label = {'angle':['user_angle']}, num_parallel_calls = 3, image_shape = (120, 160, 3)):\n",
        "    self.input_label = input_label\n",
        "    self.output_label = output_label\n",
        "    \n",
        "    self.num_parallel_calls = num_parallel_calls\n",
        "    self.image_shape = image_shape\n",
        "  \n",
        "  def dataset_to_tensor(self, dataset):\n",
        "    \"\"\"\n",
        "    {\"input\" : dataset['path'], \"speed_accel_gyro\" : dataset[['speed', 'accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']]}, {\"angle\" : dataset['angle']}\n",
        "    :return: Tensor\n",
        "    \"\"\"\n",
        "    input_dict = {}\n",
        "    output_dict = {}\n",
        "\n",
        "    # Inputs\n",
        "    for k, l in self.input_label.items():\n",
        "      if len(l) != 0:\n",
        "        if len(l) == 1:\n",
        "          input_dict[k] = dataset[l[0]]\n",
        "        else:\n",
        "          input_dict[k] = dataset[l]\n",
        "\n",
        "    # Outputs\n",
        "    for k, l in self.output_label.items():\n",
        "      if len(l) != 0:\n",
        "        if len(l) == 1:\n",
        "          output_dict[k] = dataset[l[0]]\n",
        "        else:\n",
        "          output_dict[k] = dataset[l]\n",
        "    return tf.data.Dataset.from_tensor_slices((input_dict, output_dict))\n",
        "  \n",
        "  def load_image(self, dataset_tensor):\n",
        "    def load_image_map_func(inputs, outputs):\n",
        "      loaded_inputs = dict(inputs)\n",
        "\n",
        "      img = tf.io.read_file(inputs['input'])\n",
        "      img = tf.cast(tf.image.decode_jpeg(img, channels=3), dtype=tf.float32)\n",
        "      img = tf.reshape(img, self.image_shape)\n",
        "      loaded_inputs['input'] = img\n",
        "\n",
        "      return loaded_inputs, outputs\n",
        "    return dataset_tensor.map(load_image_map_func, num_parallel_calls = self.num_parallel_calls)\n",
        "  \n",
        "  def make_augmentation(self, dataset_tensor, ratio_augmentation = 0.5, ratio_flip_left_right = 0.5, max_brightness = 50,\n",
        "                  lower_contrast = 0.75, upper_contrast = 1.5, lower_saturation = 0.0, \n",
        "                  upper_saturation = 2, mean_noise = 0.0, max_noise = 0.3):\n",
        "    def augmentation_map_func(inputs, outputs):\n",
        "      transformed_inputs = dict(inputs)\n",
        "      transformed_outputs = dict(outputs)\n",
        "      img = inputs['input']\n",
        "      angle = outputs['angle']\n",
        "\n",
        "      transformed_img, transformed_angle = DonkeyCarDataAugmentator.transform(img, \n",
        "                                                                  angle = angle, \n",
        "                                                                  ratio_augmentation = ratio_augmentation, \n",
        "                                                                  ratio_flip_left_right = ratio_flip_left_right, \n",
        "                                                                  max_brightness = max_brightness,\n",
        "                                                                  lower_contrast = lower_contrast, \n",
        "                                                                  upper_contrast = upper_contrast, \n",
        "                                                                  lower_saturation = lower_saturation, \n",
        "                                                                  upper_saturation = upper_saturation, \n",
        "                                                                  mean_noise = mean_noise, \n",
        "                                                                  max_noise = max_noise)\n",
        "\n",
        "      transformed_img = tf.reshape(transformed_img, self.image_shape)\n",
        "      transformed_inputs['input'] = transformed_img\n",
        "      transformed_outputs['angle'] = transformed_angle\n",
        "      return transformed_inputs, transformed_outputs\n",
        "    return dataset_tensor.map(augmentation_map_func, num_parallel_calls = self.num_parallel_calls)\n",
        "  \n",
        "  def normalize_dataset(self, dataset_tensor):\n",
        "    def normalize_map_func(inputs, outputs):\n",
        "      transformed_inputs = dict(inputs)\n",
        "      transformed_img = inputs['input']\n",
        "      transformed_img = DonkeyCarDataAugmentator.normalize(transformed_img)\n",
        "      transformed_img = tf.reshape(transformed_img, self.image_shape)\n",
        "      transformed_inputs['input'] = transformed_img\n",
        "      return transformed_inputs, outputs\n",
        "    return dataset_tensor.map(normalize_map_func, num_parallel_calls = self.num_parallel_calls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlHwo8uuijLj"
      },
      "source": [
        "#### Préparer la dataset\n",
        "Shape des IO, Convertir en Tensor, Preprocess, Augmentation possiblement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK7O9CKKj_bR"
      },
      "source": [
        "### <<< CONFIG >>> ###\n",
        "input_label = {'input':['path'], 'speed_accel_gyro':['speed', 'accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']}\n",
        "output_label = {'angle':['user_angle']}\n",
        "\n",
        "tensor_builder = DonkeyCarTensorBuilder(input_label = input_label,\n",
        "                                        output_label = output_label,\n",
        "                                        num_parallel_calls = 3,\n",
        "                                        image_shape = IMAGE_SHAPE)\n",
        "\n",
        "# Transforme panda DataFrame en Tensor\n",
        "train_tensor = tensor_builder.dataset_to_tensor(train_set)\n",
        "test_tensor = tensor_builder.dataset_to_tensor(test_set)\n",
        "validation_tensor = tensor_builder.dataset_to_tensor(validation_set)\n",
        "\n",
        "# Charger les images, i.e lire les path et stocker les images à la place\n",
        "train_tensor = tensor_builder.load_image(train_tensor)\n",
        "test_tensor = tensor_builder.load_image(test_tensor)\n",
        "validation_tensor = tensor_builder.load_image(validation_tensor)\n",
        "\n",
        "# Mélanger, répéter, faire des batch et activer le pré-traitement\n",
        "# On répète train_tensor pour faire de l'augmentation\n",
        "train_tensor = train_tensor.shuffle(NBR_ROW_TRAIN_SET)#.repeat(2) ### <<< CONFIG >>> (repeat) ###\n",
        "test_tensor = test_tensor.shuffle(NBR_ROW_TEST_SET)\n",
        "validation_tensor = validation_tensor.shuffle(NBR_ROW_VALIDATION_SET)\n",
        "\n",
        "### <<< CONFIG >>> (comment or not) ###\n",
        "# Augmentation\n",
        "\"\"\"\n",
        "train_tensor = tensor_builder.make_augmentation(train_tensor, \n",
        "                                                ratio_augmentation = 0.5, \n",
        "                                                ratio_flip_left_right = 0.5, \n",
        "                                                max_brightness = 50,\n",
        "                                                lower_contrast = 0.75, \n",
        "                                                upper_contrast = 1.5, \n",
        "                                                lower_saturation = 0.0, \n",
        "                                                upper_saturation = 2, \n",
        "                                                mean_noise = 0.0, \n",
        "                                                max_noise = 0.3)\n",
        "\"\"\"\n",
        "# Normaliser les images\n",
        "train_tensor = tensor_builder.normalize_dataset(train_tensor).batch(BATCH_SIZE).prefetch(2)\n",
        "test_tensor = tensor_builder.normalize_dataset(test_tensor).batch(BATCH_SIZE).prefetch(2)\n",
        "validation_tensor = tensor_builder.normalize_dataset(validation_tensor).batch(BATCH_SIZE).prefetch(2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}