{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCTrainer_V5-alpha.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PkUZho8ERPFR",
        "S9ExsLYHVhCY",
        "7cFeLMGeVwiY",
        "4RXODUFqWd7n",
        "F49jCa4fW_m6",
        "jdadli4NbNNg",
        "YctDhHb2bWcj",
        "zpLFvnIBba_A",
        "ubbUR7U5boZv",
        "WX5FwQujcA-B",
        "QAVdDXrhceYS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJc2Zr05PezS"
      },
      "source": [
        "Donkey Car Tainer V5 *Alpha*\n",
        "----------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkUZho8ERPFR"
      },
      "source": [
        "## Avant propos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ExsLYHVhCY"
      },
      "source": [
        "#### Lister les GPU disponibles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pZarMsXVkqF"
      },
      "source": [
        "import tensorflow\n",
        "print(tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtxKQ8FTVuC0"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "print(get_available_gpus())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cFeLMGeVwiY"
      },
      "source": [
        "#### Choisir le GPU de lancement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bg0Vn3cVDKU"
      },
      "source": [
        "- Avec la variable d'environnement `CUDA_VISIBLE_DEVICES`, asigner la valeur :\n",
        "  * `-1` pour faire du calcul sur CPU\n",
        "  * `0` ou `1` ou ... pour faire du calcul respectivement sur GPU 0, 1 ou ...\n",
        "  * `0,1` pour faire du calcul sur les 2 GPU 0 et GPU 1.\n",
        "\n",
        "- Si `CUDA_VISIBLE_DEVICES` ne fonctionne pas,\n",
        "on peut encadrer le fit avec ce code :\n",
        "```\n",
        "with tf.device(\"/gpu:1\"):\n",
        "    model.fit(...)\n",
        "```\n",
        "pour lancer sur le GPU 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-X8eo7CU6u9"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w1yMYAUQsSB"
      },
      "source": [
        "## Importer les librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz27lSPGPMDR"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from PIL import Image\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import inspect\n",
        "from matplotlib import pyplot as plt\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RXODUFqWd7n"
      },
      "source": [
        "## Importer la dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F49jCa4fW_m6"
      },
      "source": [
        "### Depuis Colab via Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjtcYCQ0W-Kc"
      },
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c33tTwZ8XaNz"
      },
      "source": [
        "!rm -Rf \"corentin_renault_20000_record_controller\"\n",
        "!cp \"drive/My Drive/ColabStorage/DonkeyCar/Simulator/Dataset/corentin_renault_20000_record_controller.eslr\" \"dataset.eslr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbqbGWrXv28"
      },
      "source": [
        "### Autre source via CURL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAuXDFszXbfz"
      },
      "source": [
        "!curl <LIEN_URL> --output dataset.eslr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O1j1jzTYAHY"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UVlhD7gbEGL"
      },
      "source": [
        "TIME = str(time())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdadli4NbNNg"
      },
      "source": [
        "### Environement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvKreHPDbSj3"
      },
      "source": [
        "### ENVIRONEMENT ###\n",
        "STORAGE_ROOT_DIR = os.path.abspath(\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YctDhHb2bWcj"
      },
      "source": [
        "### Modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYGz9WCFbYsk"
      },
      "source": [
        "### MODEL ###\n",
        "MODEL_NAME = \"DCDeepModelV5.0-reda-renault-speed_accel_gyro-\" + TIME\n",
        "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
        "os.environ[\"MODEL_NAME_TAR\"] = MODEL_NAME+\".tar.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLFvnIBba_A"
      },
      "source": [
        "### Sauvegarde"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DrIGM1XbmD3"
      },
      "source": [
        "### SAVE PATH ###\n",
        "SAVE_PATH = os.path.join(STORAGE_ROOT_DIR, \"model\", MODEL_NAME)\n",
        "os.environ[\"SAVE_PATH\"] = SAVE_PATH\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Don't remove the last \"s\" in \"checkpoints\",\n",
        "# the file `checkpoint` already exists\n",
        "CHECKPOINT_PATH = os.path.join(SAVE_PATH, \"checkpoints\")\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "CHECKPOINT_FILEPATH = os.path.join(CHECKPOINT_PATH, \"checkpoint-{epoch:02d}.weight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubbUR7U5boZv"
      },
      "source": [
        "### Log d'entraînement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjTOmMHBby9-"
      },
      "source": [
        "### LOG ###\n",
        "ROOT_TENSORLOG_PATH = os.path.join(STORAGE_ROOT_DIR, \"log\", MODEL_NAME)\n",
        "os.makedirs(ROOT_TENSORLOG_PATH, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i-QICSobzZz"
      },
      "source": [
        "def get_new_tensorlog_path():\n",
        "    special_log_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    special_log_path = os.path.join(ROOT_TENSORLOG_PATH, special_log_name)\n",
        "    os.makedirs(special_log_path)\n",
        "    print(build_log_tag(LOG_PATH=LOG_PATH))\n",
        "    os.environ[\"LOG_PATH\"] = special_log_path\n",
        "    return special_log_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX5FwQujcA-B"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyGaEHvPcDLS"
      },
      "source": [
        "### DATASET ###\n",
        "DATASET_NAME = \"corentin_renault_30000_clean_record_controller\"\n",
        "DATASET_FILE_PATH = os.path.join(STORAGE_ROOT_DIR, \"sample\", DATASET_NAME + \".eslr\")\n",
        "\n",
        "IMAGE_PATH = \"images\"\n",
        "DATASET_LABEL_FILENAME = \"label.csv\"\n",
        "\n",
        "DATASET_LABEL_PATH = os.path.join(DATASET_NAME, DATASET_LABEL_FILENAME)\n",
        "DATASET_IMAGE_PATH = os.path.join(DATASET_NAME, IMAGE_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qc7D5BqcY2h"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "SPLIT_VALIDATION = 0.05\n",
        "SPLIT_TEST = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAVdDXrhceYS"
      },
      "source": [
        "### Entrée du réseau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo1Tw6qpcsj4"
      },
      "source": [
        "IMAGE_SHAPE = (120,160, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvbR5ssYc7PU"
      },
      "source": [
        "### Résumé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC0bQeDwdzOT"
      },
      "source": [
        "def build_log_tag(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Generate a string as a tag to parse the logs more easily\n",
        "    \n",
        "    If you call `build_log_tag(\"arg1\", \"arg2\", key1=\"value1\", key2=\"value2\")`\n",
        "    This function generate this string in return :\n",
        "    [arg1][arg2][key1=\"value1\"][key2=\"value2\"]\n",
        "    \"\"\"\n",
        "    generated_string = \"\"\n",
        "    for v in args:\n",
        "        generated_string += \"[\" + str(v) + \"]\"\n",
        "    \n",
        "    for k,v in kwargs.items():\n",
        "        generated_string += \"[\" + str(k) + \"=\" + \"\\\"\" + str(v) + \"\\\"]\"\n",
        "    \n",
        "    return generated_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD9-h62MYGJt"
      },
      "source": [
        "print(build_log_tag(TIME=TIME))\n",
        "print(build_log_tag(STORAGE_ROOT_DIR=STORAGE_ROOT_DIR))\n",
        "print(build_log_tag(MODEL_NAME=MODEL_NAME))\n",
        "print(build_log_tag(SAVE_PATH=SAVE_PATH))\n",
        "print(build_log_tag(CHECKPOINT_PATH=CHECKPOINT_PATH))\n",
        "print(build_log_tag(CHECKPOINT_FILEPATH=CHECKPOINT_FILEPATH))\n",
        "print(build_log_tag(ROOT_TENSORLOG_PATH=ROOT_TENSORLOG_PATH))\n",
        "print(build_log_tag(DATASET_NAME=DATASET_NAME))\n",
        "print(build_log_tag(DATASET_FILE_PATH=DATASET_FILE_PATH))\n",
        "print(build_log_tag(IMAGE_PATH=IMAGE_PATH))\n",
        "print(build_log_tag(DATASET_LABEL_FILENAME=DATASET_LABEL_FILENAME))\n",
        "print(build_log_tag(DATASET_LABEL_PATH=DATASET_LABEL_PATH))\n",
        "print(build_log_tag(DATASET_IMAGE_PATH=DATASET_IMAGE_PATH))\n",
        "print(build_log_tag(BATCH_SIZE=BATCH_SIZE))\n",
        "print(build_log_tag(SPLIT_VALIDATION=SPLIT_VALIDATION))\n",
        "print(build_log_tag(SPLIT_TEST=SPLIT_TEST))\n",
        "print(build_log_tag(IMAGE_SHAPE=IMAGE_SHAPE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6nfD7xgiAC"
      },
      "source": [
        "## Extraire la dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmE-7IUUgtGE"
      },
      "source": [
        "On convertit chaque ligne du fichier *.eslr envoyées par le serveur en :\n",
        "- une image qui sera stockée dans le dossier `<DATASET_NAME>/<DATASET_IMAGE_PATH>`\n",
        "- une ligne dans le csv label.csv avec toutes les infos (reliées à aux images par leur `path`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJnXVBcgvAV"
      },
      "source": [
        "class ESLRExtractor:\n",
        "  def __init__(self, eslr_path):\n",
        "    self.eslr_path = eslr_path\n",
        "    if not os.path.exists(self.eslr_path):\n",
        "      raise Exception(\"ESLR File not found !\")\n",
        "  \n",
        "  def extract(self, label_path, images_path, image_ext = \".jpeg\"):\n",
        "    if os.path.exists(images_path):\n",
        "        print(\"[INFO] .eslr is already extracted !\")\n",
        "        return\n",
        "    # Créer le dossier qui contiendra toutes les images extraites du .eslr s'il n'existe pas\n",
        "    os.makedirs(images_path, exist_ok=True)\n",
        "\n",
        "    # Ouvrir le fichier label.csv\n",
        "    label_file = open(label_path, \"w\")\n",
        "\n",
        "    # Pour définir les en-têtes du fichier label, il faut lire au moins la première ligne\n",
        "    # du fichier *.eslr\n",
        "    label_head_is_defined = False\n",
        "\n",
        "    # Lire le fichier eslr\n",
        "    with open(self.eslr_path, \"r\") as dataset_file:\n",
        "      for i, line in enumerate(tqdm(dataset_file)):\n",
        "        data_line = json.loads(line)\n",
        "        if (data_line[\"msg_type\"] == \"telemetry\"):\n",
        "          # Si le header n'a pas encore initialisé\n",
        "          if not label_head_is_defined:\n",
        "            label_head_list = list(data_line.keys())\n",
        "            label_head_list.remove(\"msg_type\")\n",
        "            label_head_list.remove(\"image\")\n",
        "            label_head_list = ['path'] + label_head_list\n",
        "            label_head_str = \",\".join(label_head_list)\n",
        "            # Écrire le header dans le CSV\n",
        "            label_file.write(label_head_str + \"\\n\")\n",
        "            label_head_is_defined = True\n",
        "          # Définir le path de l'image à enregistrer\n",
        "          image_focused_path = os.path.join(images_path, str(i) + image_ext)\n",
        "          data_line['path'] = image_focused_path\n",
        "          # Lire, décoder et enregistrer l'image\n",
        "          Image.open(BytesIO(base64.b64decode(data_line[\"image\"]))).save(image_focused_path)\n",
        "          # Ajouter toutes les données de la ligne lue dans un le CSV\n",
        "          # Mettre 0 comme valeur par défaut si la valeur n'est pas trouvée dans data_line\n",
        "          data_list_to_write = [str(data_line.get(k, 0)) for k in label_head_list]\n",
        "          label_file.write(\",\".join(data_list_to_write) + \"\\n\")\n",
        "    label_file.close()\n",
        "  \n",
        "  @staticmethod\n",
        "  def read_csv(images_path):\n",
        "    return pd.read_csv(images_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKWPJMf7g1Mj"
      },
      "source": [
        "eslr_extractor = ESLRExtractor(DATASET_FILE_PATH)\n",
        "eslr_extractor.extract(label_path = DATASET_LABEL_PATH, images_path = DATASET_IMAGE_PATH)\n",
        "raw_data = eslr_extractor.read_csv(DATASET_LABEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA32IFleg2z1"
      },
      "source": [
        "raw_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiWJF3Z8g_aR"
      },
      "source": [
        "raw_data.hist(figsize=(20,20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IYJ17buhEFN"
      },
      "source": [
        "## Préparer la dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tya6PW9yhjcn"
      },
      "source": [
        "### Split en 3 jeux : Train, Test et Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLAJtdPJhTTJ"
      },
      "source": [
        "train_and_test_set, validation_set = train_test_split(raw_data,\n",
        "                                             test_size = SPLIT_VALIDATION,\n",
        "                                             shuffle = True)\n",
        "train_set, test_set = train_test_split(train_and_test_set,\n",
        "                                             test_size = SPLIT_TEST,\n",
        "                                             shuffle = True)\n",
        "\n",
        "NBR_ROW_TRAIN_SET = train_set.shape[0]\n",
        "NBR_ROW_TEST_SET = test_set.shape[0]\n",
        "NBR_ROW_VALIDATION_SET = validation_set.shape[0]\n",
        "print(train_set)\n",
        "print(test_set)\n",
        "print(validation_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhzCiSAFhtnO"
      },
      "source": [
        "### Traitements avec TensorData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6rkClSiXEl"
      },
      "source": [
        "#### Donkey Car Data Augmentator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIK9CMehwc2"
      },
      "source": [
        "class DonkeyCarDataAugmentator:\n",
        "  @staticmethod\n",
        "  def normalize(img):\n",
        "    return (img / 127.5) - 1.0\n",
        "  \n",
        "  @staticmethod\n",
        "  def unnormalize(img):\n",
        "    return (img + 1.0) * 127.5\n",
        "\n",
        "  @staticmethod\n",
        "  def clip_image(img):\n",
        "    return tf.clip_by_value(img, clip_value_min=0, clip_value_max=255)\n",
        "\n",
        "  @staticmethod\n",
        "  def noiser(img, mean, stddev):\n",
        "    transformed_img = DonkeyCarDataAugmentator.normalize(img)\n",
        "    noise_img = tf.random.normal(shape=tf.shape(img), mean=mean, stddev=stddev)\n",
        "    transformed_img = tf.add(transformed_img, noise_img)\n",
        "    transformed_img = DonkeyCarDataAugmentator.unnormalize(transformed_img)\n",
        "    transformed_img = DonkeyCarDataAugmentator.clip_image(transformed_img)\n",
        "    return transformed_img\n",
        "\n",
        "  @staticmethod\n",
        "  def transform(img, angle, ratio_augmentation = 0.75, ratio_flip_left_right = 0.5, max_brightness = 50,\n",
        "                lower_contrast = 0.75, upper_contrast = 1.5, lower_saturation = 0.0, \n",
        "                upper_saturation = 2, mean_noise = 0.0, max_noise = 0.3):\n",
        "    \n",
        "    random_do_augmentation = tf.random.uniform(shape=[], minval = 0., maxval = 1., dtype=tf.float32)\n",
        "    if random_do_augmentation <= ratio_augmentation:\n",
        "      transformed_img = tf.image.random_brightness(img, max_delta = max_brightness)\n",
        "      transformed_img = DonkeyCarDataAugmentator.clip_image(transformed_img)\n",
        "\n",
        "      transformed_img = tf.image.random_contrast(transformed_img, lower = lower_contrast, upper = upper_contrast)\n",
        "      transformed_img = DonkeyCarDataAugmentator.clip_image(transformed_img)\n",
        "      \n",
        "      transformed_img = tf.image.random_saturation(transformed_img, lower = lower_saturation, upper = upper_saturation)\n",
        "      transformed_img = DonkeyCarDataAugmentator.clip_image(transformed_img)\n",
        "\n",
        "      random_noise_gain = tf.random.uniform(shape=[], minval = 0.0, maxval = max_noise, dtype=tf.float32)\n",
        "      transformed_img = DonkeyCarDataAugmentator.noiser(transformed_img, mean_noise, random_noise_gain)\n",
        "    else:\n",
        "      transformed_img = img\n",
        "    \n",
        "    random_do_flip = tf.random.uniform(shape=[], minval = 0., maxval = 1., dtype=tf.float32)\n",
        "    if random_do_flip <= ratio_flip_left_right:\n",
        "      transformed_img = tf.image.flip_left_right(transformed_img)\n",
        "      angle *= -1\n",
        "    \n",
        "    return transformed_img, angle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weSexOwciear"
      },
      "source": [
        "#### Donkey Car Tensor Builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLRvD9K5igX1"
      },
      "source": [
        "class DonkeyCarTensorBuilder:\n",
        "  def __init__(self, input_label = {'input':['path']}, output_label = {'angle':['user_angle']}, num_parallel_calls = 3, image_shape = (120, 160, 3)):\n",
        "    self.input_label = input_label\n",
        "    self.output_label = output_label\n",
        "    \n",
        "    self.num_parallel_calls = num_parallel_calls\n",
        "    self.image_shape = image_shape\n",
        "  \n",
        "  def dataset_to_tensor(self, dataset):\n",
        "    \"\"\"\n",
        "    {\"input\" : dataset['path'], \"speed_accel_gyro\" : dataset[['speed', 'accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']]}, {\"angle\" : dataset['angle']}\n",
        "    :return: Tensor\n",
        "    \"\"\"\n",
        "    input_dict = {}\n",
        "    output_dict = {}\n",
        "\n",
        "    # Inputs\n",
        "    for k, l in self.input_label.items():\n",
        "      if len(l) != 0:\n",
        "        if len(l) == 1:\n",
        "          input_dict[k] = dataset[l[0]]\n",
        "        else:\n",
        "          input_dict[k] = dataset[l]\n",
        "\n",
        "    # Outputs\n",
        "    for k, l in self.output_label.items():\n",
        "      if len(l) != 0:\n",
        "        if len(l) == 1:\n",
        "          output_dict[k] = dataset[l[0]]\n",
        "        else:\n",
        "          output_dict[k] = dataset[l]\n",
        "    return tf.data.Dataset.from_tensor_slices((input_dict, output_dict))\n",
        "  \n",
        "  def load_image(self, dataset_tensor):\n",
        "    def load_image_map_func(inputs, outputs):\n",
        "      loaded_inputs = dict(inputs)\n",
        "\n",
        "      img = tf.io.read_file(inputs['input'])\n",
        "      img = tf.cast(tf.image.decode_jpeg(img, channels=3), dtype=tf.float32)\n",
        "      img = tf.reshape(img, self.image_shape)\n",
        "      loaded_inputs['input'] = img\n",
        "\n",
        "      return loaded_inputs, outputs\n",
        "    return dataset_tensor.map(load_image_map_func, num_parallel_calls = self.num_parallel_calls)\n",
        "  \n",
        "  def make_augmentation(self, dataset_tensor, ratio_augmentation = 0.5, ratio_flip_left_right = 0.5, max_brightness = 50,\n",
        "                  lower_contrast = 0.75, upper_contrast = 1.5, lower_saturation = 0.0, \n",
        "                  upper_saturation = 2, mean_noise = 0.0, max_noise = 0.3):\n",
        "    def augmentation_map_func(inputs, outputs):\n",
        "      transformed_inputs = dict(inputs)\n",
        "      transformed_outputs = dict(outputs)\n",
        "      img = inputs['input']\n",
        "      angle = outputs['angle']\n",
        "\n",
        "      transformed_img, transformed_angle = DonkeyCarDataAugmentator.transform(img, \n",
        "                                                                  angle = angle, \n",
        "                                                                  ratio_augmentation = ratio_augmentation, \n",
        "                                                                  ratio_flip_left_right = ratio_flip_left_right, \n",
        "                                                                  max_brightness = max_brightness,\n",
        "                                                                  lower_contrast = lower_contrast, \n",
        "                                                                  upper_contrast = upper_contrast, \n",
        "                                                                  lower_saturation = lower_saturation, \n",
        "                                                                  upper_saturation = upper_saturation, \n",
        "                                                                  mean_noise = mean_noise, \n",
        "                                                                  max_noise = max_noise)\n",
        "\n",
        "      transformed_img = tf.reshape(transformed_img, self.image_shape)\n",
        "      transformed_inputs['input'] = transformed_img\n",
        "      transformed_outputs['angle'] = transformed_angle\n",
        "      return transformed_inputs, transformed_outputs\n",
        "    return dataset_tensor.map(augmentation_map_func, num_parallel_calls = self.num_parallel_calls)\n",
        "  \n",
        "  def normalize_dataset(self, dataset_tensor):\n",
        "    def normalize_map_func(inputs, outputs):\n",
        "      transformed_inputs = dict(inputs)\n",
        "      transformed_img = inputs['input']\n",
        "      transformed_img = DonkeyCarDataAugmentator.normalize(transformed_img)\n",
        "      transformed_img = tf.reshape(transformed_img, self.image_shape)\n",
        "      transformed_inputs['input'] = transformed_img\n",
        "      return transformed_inputs, outputs\n",
        "    return dataset_tensor.map(normalize_map_func, num_parallel_calls = self.num_parallel_calls)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlHwo8uuijLj"
      },
      "source": [
        "#### Préparer la dataset\n",
        "Shape des IO, Convertir en Tensor, Preprocess, Augmentation possiblement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK7O9CKKj_bR"
      },
      "source": [
        "### <<< CONFIG >>> ###\n",
        "input_label = {'input':['path'], 'speed_accel_gyro':['speed', 'accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']}\n",
        "output_label = {'angle':['user_angle']}\n",
        "\n",
        "tensor_builder = DonkeyCarTensorBuilder(input_label = input_label,\n",
        "                                        output_label = output_label,\n",
        "                                        num_parallel_calls = 3,\n",
        "                                        image_shape = IMAGE_SHAPE)\n",
        "\n",
        "# Transforme panda DataFrame en Tensor\n",
        "train_tensor = tensor_builder.dataset_to_tensor(train_set)\n",
        "test_tensor = tensor_builder.dataset_to_tensor(test_set)\n",
        "validation_tensor = tensor_builder.dataset_to_tensor(validation_set)\n",
        "\n",
        "# Charger les images, i.e lire les path et stocker les images à la place\n",
        "train_tensor = tensor_builder.load_image(train_tensor)\n",
        "test_tensor = tensor_builder.load_image(test_tensor)\n",
        "validation_tensor = tensor_builder.load_image(validation_tensor)\n",
        "\n",
        "# Mélanger, répéter, faire des batch et activer le pré-traitement\n",
        "# On répète train_tensor pour faire de l'augmentation\n",
        "train_tensor = train_tensor.shuffle(NBR_ROW_TRAIN_SET)#.repeat(2) ### <<< CONFIG >>> (repeat) ###\n",
        "test_tensor = test_tensor.shuffle(NBR_ROW_TEST_SET)\n",
        "validation_tensor = validation_tensor.shuffle(NBR_ROW_VALIDATION_SET)\n",
        "\n",
        "### <<< CONFIG >>> (comment or not) ###\n",
        "# Augmentation\n",
        "\"\"\"\n",
        "train_tensor = tensor_builder.make_augmentation(train_tensor, \n",
        "                                                ratio_augmentation = 0.5, \n",
        "                                                ratio_flip_left_right = 0.5, \n",
        "                                                max_brightness = 50,\n",
        "                                                lower_contrast = 0.75, \n",
        "                                                upper_contrast = 1.5, \n",
        "                                                lower_saturation = 0.0, \n",
        "                                                upper_saturation = 2, \n",
        "                                                mean_noise = 0.0, \n",
        "                                                max_noise = 0.3)\n",
        "\"\"\"\n",
        "# Normaliser les images\n",
        "train_tensor = tensor_builder.normalize_dataset(train_tensor).batch(BATCH_SIZE).prefetch(2)\n",
        "test_tensor = tensor_builder.normalize_dataset(test_tensor).batch(BATCH_SIZE).prefetch(2)\n",
        "validation_tensor = tensor_builder.normalize_dataset(validation_tensor).batch(BATCH_SIZE).prefetch(2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}